import os
import logging
from fastapi import FastAPI, HTTPException, Query, Request, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, Response
from typing import List, Optional, Dict, Any, Literal
import json
from datetime import datetime
from pydantic import BaseModel, Field
import google.generativeai as genai
import asyncio

# Configure logging
from api.logging_config import setup_logging

setup_logging()
logger = logging.getLogger(__name__)


# Initialize FastAPI app
app = FastAPI(
    title="Streaming API",
    description="API for streaming chat completions"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

# Helper function to get adalflow root path
def get_adalflow_default_root_path():
    return os.path.expanduser(os.path.join("~", ".adalflow"))

# --- Pydantic Models ---
class WikiPage(BaseModel):
    """
    Model for a wiki page.
    """
    id: str
    title: str
    content: str
    filePaths: List[str]
    importance: str # Should ideally be Literal['high', 'medium', 'low']
    relatedPages: List[str]

class ProcessedProjectEntry(BaseModel):
    id: str  # Filename
    owner: str
    repo: str
    name: str  # owner/repo
    repo_type: str # Renamed from type to repo_type for clarity with existing models
    submittedAt: int # Timestamp
    language: str # Extracted from filename

class RepoInfo(BaseModel):
    owner: str
    repo: str
    type: str
    token: Optional[str] = None
    localPath: Optional[str] = None
    repoUrl: Optional[str] = None


class WikiSection(BaseModel):
    """
    Model for the wiki sections.
    """
    id: str
    title: str
    pages: List[str]
    subsections: Optional[List[str]] = None


class WikiStructureModel(BaseModel):
    """
    Model for the overall wiki structure.
    """
    id: str
    title: str
    description: str
    pages: List[WikiPage]
    sections: Optional[List[WikiSection]] = None
    rootSections: Optional[List[str]] = None

class WikiCacheData(BaseModel):
    """
    Model for the data to be stored in the wiki cache.
    """
    wiki_structure: WikiStructureModel
    generated_pages: Dict[str, WikiPage]
    repo_url: Optional[str] = None  #compatible for old cache
    repo: Optional[RepoInfo] = None
    provider: Optional[str] = None
    model: Optional[str] = None

class WikiCacheRequest(BaseModel):
    """
    Model for the request body when saving wiki cache.
    """
    repo: RepoInfo
    language: str
    wiki_structure: WikiStructureModel
    generated_pages: Dict[str, WikiPage]
    provider: str
    model: str

class WikiExportRequest(BaseModel):
    """
    Model for requesting a wiki export.
    """
    repo_url: str = Field(..., description="URL of the repository")
    pages: List[WikiPage] = Field(..., description="List of wiki pages to export")
    format: Literal["markdown", "json"] = Field(..., description="Export format (markdown or json)")

# --- Model Configuration Models ---
class Model(BaseModel):
    """
    Model for LLM model configuration
    """
    id: str = Field(..., description="Model identifier")
    name: str = Field(..., description="Display name for the model")

class Provider(BaseModel):
    """
    Model for LLM provider configuration
    """
    id: str = Field(..., description="Provider identifier")
    name: str = Field(..., description="Display name for the provider")
    models: List[Model] = Field(..., description="List of available models for this provider")
    supportsCustomModel: Optional[bool] = Field(False, description="Whether this provider supports custom models")

class ModelConfig(BaseModel):
    """
    Model for the entire model configuration
    """
    providers: List[Provider] = Field(..., description="List of available model providers")
    defaultProvider: str = Field(..., description="ID of the default provider")

class AuthorizationConfig(BaseModel):
    code: str = Field(..., description="Authorization code")

from api.config import configs, WIKI_AUTH_MODE, WIKI_AUTH_CODE

@app.get("/lang/config")
async def get_lang_config():
    return configs["lang_config"]

@app.get("/auth/status")
async def get_auth_status():
    """
    Check if authentication is required for the wiki.
    """
    return {"auth_required": WIKI_AUTH_MODE}

@app.post("/auth/validate")
async def validate_auth_code(request: AuthorizationConfig):
    """
    Check authorization code.
    """
    return {"success": WIKI_AUTH_CODE == request.code}

@app.get("/models/config", response_model=ModelConfig)
async def get_model_config():
    """
    Get available model providers and their models.

    This endpoint returns the configuration of available model providers and their
    respective models that can be used throughout the application.

    Returns:
        ModelConfig: A configuration object containing providers and their models
    """
    try:
        logger.info("Fetching model configurations")

        # Create providers from the config file
        providers = []
        default_provider = configs.get("default_provider", "google")

        # Add provider configuration based on config.py
        for provider_id, provider_config in configs["providers"].items():
            models = []
            # Add models from config
            for model_id in provider_config["models"].keys():
                # Get a more user-friendly display name if possible
                models.append(Model(id=model_id, name=model_id))

            # Add provider with its models
            providers.append(
                Provider(
                    id=provider_id,
                    name=f"{provider_id.capitalize()}",
                    supportsCustomModel=provider_config.get("supportsCustomModel", False),
                    models=models
                )
            )

        # Create and return the full configuration
        config = ModelConfig(
            providers=providers,
            defaultProvider=default_provider
        )
        return config

    except Exception as e:
        logger.error(f"Error creating model configuration: {str(e)}")
        # Return some default configuration in case of error
        return ModelConfig(
            providers=[
                Provider(
                    id="google",
                    name="Google",
                    supportsCustomModel=True,
                    models=[
                        Model(id="gemini-2.5-flash", name="Gemini 2.5 Flash")
                    ]
                )
            ],
            defaultProvider="google"
        )

@app.post("/export/wiki")
async def export_wiki(request: WikiExportRequest):
    """
    Export wiki content as Markdown or JSON.

    Args:
        request: The export request containing wiki pages and format

    Returns:
        A downloadable file in the requested format
    """
    try:
        logger.info(f"Exporting wiki for {request.repo_url} in {request.format} format")

        # Extract repository name from URL for the filename
        repo_parts = request.repo_url.rstrip('/').split('/')
        repo_name = repo_parts[-1] if len(repo_parts) > 0 else "wiki"

        # Get current timestamp for the filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        if request.format == "markdown":
            # Generate Markdown content
            content = generate_markdown_export(request.repo_url, request.pages)
            filename = f"{repo_name}_wiki_{timestamp}.md"
            media_type = "text/markdown"
        else:  # JSON format
            # Generate JSON content
            content = generate_json_export(request.repo_url, request.pages)
            filename = f"{repo_name}_wiki_{timestamp}.json"
            media_type = "application/json"

        # Create response with appropriate headers for file download
        response = Response(
            content=content,
            media_type=media_type,
            headers={
                "Content-Disposition": f"attachment; filename={filename}"
            }
        )

        return response

    except Exception as e:
        error_msg = f"Error exporting wiki: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

@app.get("/local_repo/structure")
async def get_local_repo_structure(path: str = Query(None, description="æœ¬åœ°ä»“åº“è·¯å¾„")):
    """
    è¿”å›æœ¬åœ°ä»“åº“çš„æ–‡ä»¶æ ‘å’ŒREADMEå†…å®¹
    æ”¯æŒç»å¯¹è·¯å¾„å’Œç›¸å¯¹è·¯å¾„
    """
    if not path:
        return JSONResponse(
            status_code=400,
            content={"error": "æœªæä¾›è·¯å¾„ã€‚è¯·æä¾› 'path' æŸ¥è¯¢å‚æ•°ã€‚"}
        )

    try:
        # è§£æè·¯å¾„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        import pathlib
        input_path = pathlib.Path(path).expanduser().resolve()

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not input_path.exists():
            return JSONResponse(
                status_code=404,
                content={"error": f"ç›®å½•ä¸å­˜åœ¨: {path}"}
            )

        # ç¡®ä¿æ˜¯ç›®å½•
        if not input_path.is_dir():
            return JSONResponse(
                status_code=400,
                content={"error": f"è·¯å¾„ä¸æ˜¯ç›®å½•: {path}"}
            )

        logger.info(f"æ­£åœ¨å¤„ç†æœ¬åœ°ä»“åº“: {input_path}")
        file_tree_lines = []
        readme_content = ""

        # æŸ¥æ‰¾å¸¸è§çš„READMEæ–‡ä»¶å
        readme_names = ['README.md', 'readme.md', 'README.txt', 'readme.txt', 'README', 'readme']
        readme_files = []

        for root, dirs, files in os.walk(input_path):
            # æ’é™¤éšè—ç›®å½•å’Œè™šæ‹Ÿç¯å¢ƒç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__'
                      and d != 'node_modules' and d != '.venv' and d != 'venv'
                      and d != 'env' and d != '.git' and d != 'dist' and d != 'build']

            for file in files:
                # è·³è¿‡éšè—æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
                if file.startswith('.') or file in ['__init__.py', '.DS_Store', 'Thumbs.db']:
                    continue

                # è·³è¿‡å¸¸è§çš„ä¸´æ—¶å’Œç¼–è¯‘æ–‡ä»¶
                if any(file.endswith(ext) for ext in ['.pyc', '.pyo', '.pyd', '.class', '.jar', '.war']):
                    continue

                # è®¡ç®—ç›¸å¯¹è·¯å¾„
                rel_dir = os.path.relpath(root, input_path)
                if rel_dir == '.':
                    rel_file = file
                else:
                    rel_file = os.path.join(rel_dir, file)

                file_tree_lines.append(rel_file)

                # æŸ¥æ‰¾READMEæ–‡ä»¶
                if file.lower().startswith('readme'):
                    readme_files.append(os.path.join(root, file))

        # æŒ‰ä¼˜å…ˆçº§è¯»å–READMEå†…å®¹
        for readme_name in readme_names:
            for readme_file in readme_files:
                if os.path.basename(readme_file).lower() == readme_name.lower():
                    try:
                        with open(readme_file, 'r', encoding='utf-8', errors='ignore') as f:
                            readme_content = f.read()
                        logger.info(f"æˆåŠŸè¯»å–READMEæ–‡ä»¶: {readme_file}")
                        break
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–READMEæ–‡ä»¶ {readme_file}: {str(e)}")
            if readme_content:
                break

        # æ’åºæ–‡ä»¶æ ‘
        file_tree_str = '\n'.join(sorted(file_tree_lines, key=str.lower))

        return {
            "file_tree": file_tree_str,
            "readme": readme_content,
            "resolved_path": str(input_path),
            "file_count": len(file_tree_lines)
        }
    except PermissionError:
        error_msg = f"æƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®è·¯å¾„: {path}"
        logger.error(error_msg)
        return JSONResponse(
            status_code=403,
            content={"error": error_msg}
        )
    except Exception as e:
        error_msg = f"å¤„ç†æœ¬åœ°ä»“åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        return JSONResponse(
            status_code=500,
            content={"error": error_msg}
        )

@app.post("/local_repo/generate_wiki")
async def generate_local_repo_wiki(request: Dict[str, Any]):
    """
    ä¸ºæœ¬åœ°ä»“åº“ç”ŸæˆWikiå†…å®¹

    Args:
        request: åŒ…å«æœ¬åœ°è·¯å¾„å’Œæ¨¡å‹é…ç½®çš„è¯·æ±‚

    Returns:
        ç”Ÿæˆçš„Wikiç»“æ„æ•°æ®ï¼ŒåŒ…å«<wiki_structure> XMLæ ¼å¼
    """
    try:
        # æå–è¯·æ±‚å‚æ•°
        local_path = request.get("local_path")
        provider = request.get("provider", "deepseek")
        model = request.get("model", "deepseek-chat")
        language = request.get("language", "zh-CN")

        if not local_path:
            raise HTTPException(status_code=400, detail="æœªæä¾›æœ¬åœ°è·¯å¾„")

        # è§£æè·¯å¾„
        import pathlib
        input_path = pathlib.Path(local_path).expanduser().resolve()

        if not input_path.exists():
            raise HTTPException(status_code=404, detail=f"è·¯å¾„ä¸å­˜åœ¨: {local_path}")

        if not input_path.is_dir():
            raise HTTPException(status_code=400, detail=f"è·¯å¾„ä¸æ˜¯ç›®å½•: {local_path}")

        logger.info(f"æ­£åœ¨ä¸ºæœ¬åœ°ä»“åº“ç”ŸæˆWiki: {input_path}")

        # ç”ŸæˆåŸºæœ¬çš„Wikiç»“æ„ï¼ˆä¸ä¾èµ–å¤æ‚çš„æ•°æ®ç®¡é“ï¼‰
        import hashlib

        # ç”Ÿæˆå”¯ä¸€ID
        repo_name = input_path.name
        path_hash = hashlib.md5(str(input_path).encode()).hexdigest()[:8]
        wiki_id = f"local_{repo_name}_{path_hash}"

        # åˆ†ææŠ€æœ¯æ ˆ
        tech_stack = []
        try:
            for root, dirs, files in os.walk(input_path):
                for file in files:
                    if file.endswith(('.py', 'js', 'ts', 'java', 'go', 'rs')):
                        if file.endswith('.py'):
                            tech_stack.append('Python')
                        elif file.endswith(('.js', '.jsx')):
                            tech_stack.append('JavaScript')
                        elif file.endswith(('.ts', '.tsx')):
                            tech_stack.append('TypeScript')
                        elif file.endswith('.java'):
                            tech_stack.append('Java')
                        elif file.endswith('.go'):
                            tech_stack.append('Go')
                        elif file.endswith('.rs'):
                            tech_stack.append('Rust')

                        # æ£€æŸ¥é…ç½®æ–‡ä»¶
                        if file in ['package.json', 'yarn.lock']:
                            tech_stack.append('Node.js')
                        elif file in ['requirements.txt', 'setup.py', 'pyproject.toml']:
                            tech_stack.append('Python')
                        elif file in ['pom.xml', 'build.gradle']:
                            tech_stack.append('Java')
                        elif file in ['go.mod', 'go.sum']:
                            tech_stack.append('Go')
                        elif file in ['Cargo.toml']:
                            tech_stack.append('Rust')

                        # é™åˆ¶æ£€æŸ¥çš„æ–‡ä»¶æ•°é‡
                        if len(tech_stack) >= 5:
                            break

                if len(tech_stack) >= 5:
                    break
        except Exception as e:
            logger.warning(f"åˆ†ææŠ€æœ¯æ ˆæ—¶å‡ºé”™: {str(e)}")

        # å»é‡
        tech_stack = list(set(tech_stack))

        # ç”ŸæˆWikié¡µé¢
        wiki_pages = []

        # æ¦‚è§ˆé¡µé¢
        wiki_pages.append({
            "id": "overview",
            "title": "é¡¹ç›®æ¦‚è§ˆ",
            "content": f"# {repo_name}\n\n## é¡¹ç›®ç®€ä»‹\n\nè¿™æ˜¯ä¸€ä¸ªæœ¬åœ°é¡¹ç›®ã€‚\n\n## æŠ€æœ¯æ ˆ\n\n" + "\n".join([f"- **{tech}**" for tech in tech_stack]) + "\n\n## ç‰¹æ€§\n\n- ğŸš€ ç°ä»£åŒ–æŠ€æœ¯æ ˆ\n- ğŸ“š è¯¦ç»†æ–‡æ¡£\n- ğŸ”§ æ˜“äºé…ç½®\n- ğŸ§ª å®Œæ•´æµ‹è¯•\n\n## å¿«é€Ÿå¼€å§‹\n\nè¯·å‚è€ƒ [å®‰è£…æŒ‡å—](installation) å’Œ [ä½¿ç”¨æŒ‡å—](usage) å¼€å§‹ä½¿ç”¨æ­¤é¡¹ç›®ã€‚",
            "filePaths": [],
            "importance": "high",
            "relatedPages": ["installation", "usage"]
        })

        # å®‰è£…é¡µé¢
        install_content = "# å®‰è£…æŒ‡å—\n\n## ç¯å¢ƒè¦æ±‚\n\n"
        if "Python" in tech_stack:
            install_content += "- Python 3.8+\n"
        if "Node.js" in tech_stack or "JavaScript" in tech_stack or "TypeScript" in tech_stack:
            install_content += "- Node.js 16+\n"
        if "Java" in tech_stack:
            install_content += "- Java 8+\n"
        if "Go" in tech_stack:
            install_content += "- Go 1.19+\n"

        install_content += "\n## å®‰è£…æ­¥éª¤\n\n### 1. å…‹éš†é¡¹ç›®\n\n```bash\ngit clone <repository-url>\ncd <project-directory>\n```\n\n### 2. å®‰è£…ä¾èµ–\n\n"

        if "Python" in tech_stack:
            install_content += "```bash\npip install -r requirements.txt\n```\n\n"
        if "Node.js" in tech_stack or "JavaScript" in tech_stack or "TypeScript" in tech_stack:
            install_content += "```bash\nnpm install\n```\n\n"

        install_content += "### 3. é…ç½®ç¯å¢ƒ\n\nè¯·æ ¹æ®é¡¹ç›®éœ€è¦é…ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶ã€‚\n\n### 4. éªŒè¯å®‰è£…\n\nè¿è¡Œæµ‹è¯•æˆ–å¯åŠ¨é¡¹ç›®æ¥éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸã€‚"

        wiki_pages.append({
            "id": "installation",
            "title": "å®‰è£…æŒ‡å—",
            "content": install_content,
            "filePaths": [],
            "importance": "high",
            "relatedPages": ["usage", "overview"]
        })

        # ä½¿ç”¨æŒ‡å—é¡µé¢
        usage_content = "# ä½¿ç”¨æŒ‡å—\n\n## åŸºæœ¬ç”¨æ³•\n\n"
        if "Python" in tech_stack:
            usage_content += "```python\n# è¿è¡Œä¸»ç¨‹åº\npython main.py\n```\n\n"
        if "Node.js" in tech_stack or "JavaScript" in tech_stack or "TypeScript" in tech_stack:
            usage_content += "```bash\n# å¯åŠ¨å¼€å‘æœåŠ¡å™¨\nnpm run dev\n\n# æ„å»ºç”Ÿäº§ç‰ˆæœ¬\nnpm run build\n```\n\n"

        usage_content += "## é…ç½®é€‰é¡¹\n\né¡¹ç›®æ”¯æŒå¤šç§é…ç½®é€‰é¡¹ï¼Œè¯·å‚è€ƒé…ç½®æ–‡æ¡£äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚\n\n## æœ€ä½³å®è·µ\n\n- éµå¾ªé¡¹ç›®ç¼–ç è§„èŒƒ\n- å®šæœŸæ›´æ–°ä¾èµ–\n- ç¼–å†™æµ‹è¯•ç”¨ä¾‹\n- æŸ¥çœ‹æ—¥å¿—è¾“å‡º"

        wiki_pages.append({
            "id": "usage",
            "title": "ä½¿ç”¨æŒ‡å—",
            "content": usage_content,
            "filePaths": [],
            "importance": "high",
            "relatedPages": ["installation"]
        })

        # ç”Ÿæˆç« èŠ‚
        sections = [
            {
                "id": "getting-started",
                "title": "å¿«é€Ÿå¼€å§‹",
                "pages": ["overview", "installation", "usage"],
                "subsections": []
            }
        ]

        # ç”ŸæˆWikiç»“æ„
        wiki_structure = {
            "id": wiki_id,
            "title": f"{repo_name} Documentation",
            "description": f" Automatically generated documentation for local repository: {repo_name}",
            "pages": wiki_pages,
            "sections": sections,
            "rootSections": ["getting-started"]
        }

        # ç”Ÿæˆå®Œæ•´çš„Wikiç¼“å­˜æ•°æ®
        wiki_cache_data = {
            "wiki_structure": wiki_structure,
            "generated_pages": {},
            "repo": {
                "owner": "local",
                "repo": repo_name,
                "type": "local",
                "localPath": str(input_path)
            },
            "provider": provider,
            "model": model,
            "language": language
        }

        return wiki_cache_data

    except HTTPException:
        raise
    except Exception as e:
        error_msg = f"ç”Ÿæˆæœ¬åœ°ä»“åº“Wikiæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

@app.get("/test_local")
async def test_local():
    """
    æµ‹è¯•æœ¬åœ°è·¯å¾„å¤„ç†
    """
    return {"message": "test successful", "status": "ok"}

async def generate_wiki_structure_for_local_repo(input_path, documents, provider, model, language):
    """
    ä¸ºæœ¬åœ°ä»“åº“ç”ŸæˆWikiç»“æ„

    Args:
        input_path: æœ¬åœ°ä»“åº“è·¯å¾„
        documents: æ–‡æ¡£åˆ—è¡¨
        provider: æ¨¡å‹æä¾›å•†
        model: æ¨¡å‹åç§°
        language: è¯­è¨€

    Returns:
        Wikiç»“æ„æ•°æ®
    """
    import hashlib

    # ç”Ÿæˆå”¯ä¸€ID
    repo_name = input_path.name
    path_hash = hashlib.md5(str(input_path).encode()).hexdigest()[:8]
    wiki_id = f"local_{repo_name}_{path_hash}"

    # åˆ†æä»“åº“ç±»å‹å’Œä¸»è¦æŠ€æœ¯æ ˆ
    tech_stack = analyze_tech_stack(documents)

    # æ ¹æ®æŠ€æœ¯æ ˆç”ŸæˆWikié¡µé¢
    wiki_pages = generate_wiki_pages_for_tech_stack(repo_name, tech_stack, documents)

    # ç”ŸæˆWikiç»“æ„
    wiki_structure = {
        "id": wiki_id,
        "title": f"{repo_name} Documentation",
        "description": f" Automatically generated documentation for local repository: {repo_name}",
        "pages": wiki_pages,
        "sections": generate_wiki_sections(wiki_pages),
        "rootSections": ["overview", "installation", "usage"]
    }

    return wiki_structure

def analyze_tech_stack(documents):
    """
    åˆ†ææ–‡æ¡£ä¸­çš„æŠ€æœ¯æ ˆ

    Args:
        documents: æ–‡æ¡£åˆ—è¡¨

    Returns:
        æŠ€æœ¯æ ˆåˆ—è¡¨
    """
    tech_patterns = {
        "Python": [".py", "requirements.txt", "setup.py", "pyproject.toml"],
        "JavaScript": [".js", ".mjs", "package.json", "yarn.lock"],
        "TypeScript": [".ts", ".tsx", "tsconfig.json"],
        "React": [".jsx", "react", "next.js", "next.config.js"],
        "Java": [".java", "pom.xml", "build.gradle", "src/main"],
        "Go": [".go", "go.mod", "go.sum"],
        "Rust": [".rs", "Cargo.toml"],
        "Docker": ["Dockerfile", "docker-compose.yml", ".dockerignore"],
        "Web": [".html", ".css", ".scss", ".vue", ".svelte"],
        "C++": [".cpp", ".hpp", ".cc", ".cxx", "CMakeLists.txt", "Makefile"],
        "C#": [".cs", ".csproj", "sln"],
        "PHP": [".php", "composer.json"],
        "Ruby": [".rb", "Gemfile", "Rails"]
    }

    tech_stack = []
    doc_contents = " ".join([doc.content.lower() for doc in documents])
    doc_paths = " ".join([doc.path.lower() for doc in documents])

    for tech, patterns in tech_patterns.items():
        for pattern in patterns:
            if pattern.lower() in doc_contents or pattern.lower() in doc_paths:
                tech_stack.append(tech)
                break

    return list(set(tech_stack))

def generate_wiki_pages_for_tech_stack(repo_name, tech_stack, documents):
    """
    æ ¹æ®æŠ€æœ¯æ ˆç”ŸæˆWikié¡µé¢

    Args:
        repo_name: ä»“åº“åç§°
        tech_stack: æŠ€æœ¯æ ˆåˆ—è¡¨
        documents: æ–‡æ¡£åˆ—è¡¨

    Returns:
        Wikié¡µé¢åˆ—è¡¨
    """
    pages = []

    # æ¦‚è§ˆé¡µé¢
    pages.append({
        "id": "overview",
        "title": "é¡¹ç›®æ¦‚è§ˆ",
        "content": generate_overview_content(repo_name, tech_stack, documents),
        "filePaths": [],
        "importance": "high",
        "relatedPages": ["installation", "usage"]
    })

    # å®‰è£…é¡µé¢
    pages.append({
        "id": "installation",
        "title": "å®‰è£…æŒ‡å—",
        "content": generate_installation_content(tech_stack, documents),
        "filePaths": [],
        "importance": "high",
        "relatedPages": ["usage", "overview"]
    })

    # ä½¿ç”¨æŒ‡å—é¡µé¢
    pages.append({
        "id": "usage",
        "title": "ä½¿ç”¨æŒ‡å—",
        "content": generate_usage_content(tech_stack, documents),
        "filePaths": [],
        "importance": "high",
        "relatedPages": ["installation", "api-reference"]
    })

    # APIå‚è€ƒé¡µé¢ï¼ˆå¦‚æœæ˜¯ä»£ç é¡¹ç›®ï¼‰
    if any(tech in ["Python", "JavaScript", "TypeScript", "Java", "Go", "Rust"] for tech in tech_stack):
        pages.append({
            "id": "api-reference",
            "title": "APIå‚è€ƒ",
            "content": generate_api_content(documents),
            "filePaths": [],
            "importance": "medium",
            "relatedPages": ["usage"]
        })

    # è´¡çŒ®æŒ‡å—é¡µé¢
    pages.append({
        "id": "contributing",
        "title": "è´¡çŒ®æŒ‡å—",
        "content": generate_contributing_content(tech_stack),
        "filePaths": [],
        "importance": "low",
        "relatedPages": ["overview"]
    })

    return pages

def generate_wiki_sections(pages):
    """
    ç”ŸæˆWikiç« èŠ‚

    Args:
        pages: é¡µé¢åˆ—è¡¨

    Returns:
        ç« èŠ‚åˆ—è¡¨
    """
    sections = [
        {
            "id": "getting-started",
            "title": "å¿«é€Ÿå¼€å§‹",
            "pages": ["overview", "installation", "usage"],
            "subsections": []
        }
    ]

    # å¦‚æœæœ‰APIå‚è€ƒé¡µé¢ï¼Œæ·»åŠ APIç« èŠ‚
    if any(page["id"] == "api-reference" for page in pages):
        sections.append({
            "id": "api",
            "title": "APIæ–‡æ¡£",
            "pages": ["api-reference"],
            "subsections": []
        })

    # æ·»åŠ å…¶ä»–ç« èŠ‚
    other_pages = [page["id"] for page in pages if page["id"] not in ["overview", "installation", "usage", "api-reference"]]
    if other_pages:
        sections.append({
            "id": "additional",
            "title": "é™„åŠ ä¿¡æ¯",
            "pages": other_pages,
            "subsections": []
        })

    return sections

def generate_overview_content(repo_name, tech_stack, documents):
    """ç”Ÿæˆæ¦‚è§ˆå†…å®¹"""
    tech_badges = " ".join([f"`{tech}`" for tech in tech_stack])

    content = f"""# {repo_name}

## é¡¹ç›®ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ªåŸºäº {tech_badges} æŠ€æœ¯æ ˆçš„é¡¹ç›®ã€‚

## æŠ€æœ¯æ ˆ

"""
    for tech in tech_stack:
        content += f"- **{tech}**\n"

    content += """
## ç‰¹æ€§

- ğŸš€ ç°ä»£åŒ–æŠ€æœ¯æ ˆ
- ğŸ“š è¯¦ç»†æ–‡æ¡£
- ğŸ”§ æ˜“äºé…ç½®
- ğŸ§ª å®Œæ•´æµ‹è¯•

## å¿«é€Ÿå¼€å§‹

è¯·å‚è€ƒ [å®‰è£…æŒ‡å—](installation) å’Œ [ä½¿ç”¨æŒ‡å—](usage) å¼€å§‹ä½¿ç”¨æ­¤é¡¹ç›®ã€‚
"""

    return content

def generate_installation_content(tech_stack, documents):
    """ç”Ÿæˆå®‰è£…æŒ‡å—å†…å®¹"""
    content = """# å®‰è£…æŒ‡å—

## ç¯å¢ƒè¦æ±‚

"""

    # æ ¹æ®æŠ€æœ¯æ ˆæ·»åŠ ç¯å¢ƒè¦æ±‚
    if "Python" in tech_stack:
        content += "- Python 3.8+\n"
    if "Node.js" in tech_stack or "JavaScript" in tech_stack or "TypeScript" in tech_stack:
        content += "- Node.js 16+\n"
    if "Java" in tech_stack:
        content += "- Java 8+\n"
    if "Go" in tech_stack:
        content += "- Go 1.19+\n"

    content += """
## å®‰è£…æ­¥éª¤

### 1. å…‹éš†é¡¹ç›®

```bash
git clone <repository-url>
cd <project-directory>
```

### 2. å®‰è£…ä¾èµ–

"""

    # æ ¹æ®æŠ€æœ¯æ ˆæ·»åŠ å®‰è£…å‘½ä»¤
    if "Python" in tech_stack:
        content += """
```bash
# ä½¿ç”¨ pip
pip install -r requirements.txt

# æˆ–ä½¿ç”¨ poetry
poetry install
```
"""

    if "Node.js" in tech_stack or "JavaScript" in tech_stack or "TypeScript" in tech_stack:
        content += """
```bash
# ä½¿ç”¨ npm
npm install

# æˆ–ä½¿ç”¨ yarn
yarn install

# æˆ–ä½¿ç”¨ pnpm
pnpm install
```
"""

    if "Java" in tech_stack:
        content += """
```bash
# ä½¿ç”¨ Maven
mvn clean install

# æˆ–ä½¿ç”¨ Gradle
./gradlew build
```
"""

    content += """
### 3. é…ç½®ç¯å¢ƒ

è¯·æ ¹æ®é¡¹ç›®éœ€è¦é…ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶ã€‚

### 4. éªŒè¯å®‰è£…

è¿è¡Œæµ‹è¯•æˆ–å¯åŠ¨é¡¹ç›®æ¥éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸã€‚

## å¸¸è§é—®é¢˜

å¦‚æœåœ¨å®‰è£…è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹é¡¹ç›®çš„ FAQ æˆ–æäº¤ Issueã€‚
"""

    return content

def generate_usage_content(tech_stack, documents):
    """ç”Ÿæˆä½¿ç”¨æŒ‡å—å†…å®¹"""
    content = """# ä½¿ç”¨æŒ‡å—

## åŸºæœ¬ç”¨æ³•

"""

    # æ ¹æ®æŠ€æœ¯æ ˆæ·»åŠ ä½¿ç”¨ç¤ºä¾‹
    if "Python" in tech_stack:
        content += """
### Python ä½¿ç”¨ç¤ºä¾‹

```python
# å¯¼å…¥æ¨¡å—
from your_module import main

# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    main()
```
"""

    if "Node.js" in tech_stack or "JavaScript" in tech_stack or "TypeScript" in tech_stack:
        content += """
### Node.js ä½¿ç”¨ç¤ºä¾‹

```javascript
// å¯¼å…¥æ¨¡å—
const { main } = require('./index.js');

// è¿è¡Œä¸»å‡½æ•°
main();
```
"""

    if "React" in tech_stack:
        content += """
### React å¼€å‘

```bash
# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
npm run dev

# æ„å»ºç”Ÿäº§ç‰ˆæœ¬
npm run build

# è¿è¡Œæµ‹è¯•
npm test
```
"""

    content += """
## é…ç½®é€‰é¡¹

é¡¹ç›®æ”¯æŒå¤šç§é…ç½®é€‰é¡¹ï¼Œè¯·å‚è€ƒé…ç½®æ–‡æ¡£äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰é…ç½®

æ‚¨å¯ä»¥é€šè¿‡é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰é¡¹ç›®è¡Œä¸ºã€‚

### æ‰©å±•åŠŸèƒ½

é¡¹ç›®æ”¯æŒæ’ä»¶å’Œæ‰©å±•ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ–°åŠŸèƒ½ã€‚

## æœ€ä½³å®è·µ

- éµå¾ªé¡¹ç›®ç¼–ç è§„èŒƒ
- å®šæœŸæ›´æ–°ä¾èµ–
- ç¼–å†™æµ‹è¯•ç”¨ä¾‹
- æŸ¥çœ‹æ—¥å¿—è¾“å‡º

## æ•…éšœæ’é™¤

å¦‚æœåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
2. éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®
3. ç¡®è®¤ç¯å¢ƒè¦æ±‚
4. æŸ¥çœ‹å·²çŸ¥é—®é¢˜
"""

    return content

def generate_api_content(documents):
    """ç”ŸæˆAPIå‚è€ƒå†…å®¹"""
    content = """# API å‚è€ƒ

## æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿°äº†é¡¹ç›®çš„ä¸»è¦ API æ¥å£ã€‚

## æ ¸å¿ƒæ¨¡å—

"""

    # åˆ†ææ–‡æ¡£ä¸­çš„å‡½æ•°å’Œç±»
    functions = []
    classes = []

    for doc in documents:
        if doc.path.endswith(('.py', '.js', '.ts')):
            lines = doc.content.split('\n')
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('def ') or stripped.startswith('function '):
                    functions.append(stripped)
                elif stripped.startswith('class '):
                    classes.append(stripped)

    if functions:
        content += "### å‡½æ•°\n\n"
        for func in functions[:10]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            content += f"- `{func}`\n"
        content += "\n"

    if classes:
        content += "### ç±»\n\n"
        for cls in classes[:10]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            content += f"- `{cls}`\n"
        content += "\n"

    content += """
## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬è°ƒç”¨

```python
# ç¤ºä¾‹ä»£ç 
result = your_function(param1, param2)
print(result)
```

### é”™è¯¯å¤„ç†

```python
try:
    result = your_function(param1, param2)
except Exception as e:
    print(f"Error: {e}")
```

## å‚æ•°è¯´æ˜

è¯¦ç»†çš„å‚æ•°è¯´æ˜è¯·å‚è€ƒæºä»£ç æ³¨é‡Šã€‚

## è¿”å›å€¼

API è°ƒç”¨å°†è¿”å›ç›¸åº”çš„ç»“æœå¯¹è±¡æˆ–æ•°æ®ã€‚

## æ³¨æ„äº‹é¡¹

- è¯·æ£€æŸ¥å‚æ•°ç±»å‹å’Œæ ¼å¼
- å¤„ç†å¯èƒ½çš„å¼‚å¸¸æƒ…å†µ
- éµå¾ªè°ƒç”¨é¡ºåºè¦æ±‚
"""

    return content

def generate_contributing_content(tech_stack):
    """ç”Ÿæˆè´¡çŒ®æŒ‡å—å†…å®¹"""
    content = """# è´¡çŒ®æŒ‡å—

æ„Ÿè°¢æ‚¨å¯¹é¡¹ç›®çš„å…³æ³¨ï¼æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ã€‚

## è´¡çŒ®æ–¹å¼

### æŠ¥å‘Šé—®é¢˜

å¦‚æœæ‚¨å‘ç°äº† bug æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·ï¼š

1. æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸å…³ Issue
2. åˆ›å»ºæ–°çš„ Issue å¹¶è¯¦ç»†æè¿°
3. æä¾›é‡ç°æ­¥éª¤å’Œç¯å¢ƒä¿¡æ¯

### æäº¤ä»£ç 

#### å¼€å‘æµç¨‹

1. **Fork é¡¹ç›®**
   ```bash
   git clone <your-fork-url>
   cd <project-directory>
   ```

2. **åˆ›å»ºåˆ†æ”¯**
   ```bash
   git checkout -b feature/your-feature-name
   ```

3. **è¿›è¡Œå¼€å‘**
   - éµå¾ªä»£ç è§„èŒƒ
   - æ·»åŠ æµ‹è¯•ç”¨ä¾‹
   - æ›´æ–°æ–‡æ¡£

4. **æäº¤æ›´æ”¹**
   ```bash
   git add .
   git commit -m "feat: add your feature description"
   ```

5. **æ¨é€åˆ†æ”¯**
   ```bash
   git push origin feature/your-feature-name
   ```

6. **åˆ›å»º Pull Request**

"""

    # æ ¹æ®æŠ€æœ¯æ ˆæ·»åŠ ç‰¹å®šçš„å¼€å‘æŒ‡å—
    if "Python" in tech_stack:
        content += """
#### Python å¼€å‘æŒ‡å—

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# è¿è¡Œæµ‹è¯•
pytest

# ä»£ç æ ¼å¼åŒ–
black .
isort .

# ç±»å‹æ£€æŸ¥
mypy .
```
"""

    if "Node.js" in tech_stack or "JavaScript" in tech_stack or "TypeScript" in tech_stack:
        content += """
#### Node.js å¼€å‘æŒ‡å—

```bash
# å®‰è£…å¼€å‘ä¾èµ–
npm install --dev

# è¿è¡Œæµ‹è¯•
npm test

# ä»£ç æ£€æŸ¥
npm run lint

# æ ¼å¼åŒ–ä»£ç 
npm run format

# æ„å»ºé¡¹ç›®
npm run build
```
"""

    content += """
## ä»£ç è§„èŒƒ

### é€šç”¨è§„èŒƒ

- ä½¿ç”¨æ¸…æ™°çš„å˜é‡å’Œå‡½æ•°å‘½å
- æ·»åŠ å¿…è¦çš„æ³¨é‡Šå’Œæ–‡æ¡£
- ä¿æŒä»£ç ç®€æ´å’Œå¯è¯»æ€§
- éµå¾ªé¡¹ç›®ç°æœ‰çš„ä»£ç é£æ ¼

### æäº¤ä¿¡æ¯è§„èŒƒ

ä½¿ç”¨çº¦å®šå¼æäº¤æ ¼å¼ï¼š

- `feat:` æ–°åŠŸèƒ½
- `fix:` ä¿®å¤ bug
- `docs:` æ–‡æ¡£æ›´æ–°
- `style:` ä»£ç æ ¼å¼è°ƒæ•´
- `refactor:` ä»£ç é‡æ„
- `test:` æµ‹è¯•ç›¸å…³
- `chore:` æ„å»ºè¿‡ç¨‹æˆ–è¾…åŠ©å·¥å…·çš„å˜åŠ¨

## æµ‹è¯•è¦æ±‚

- ä¸ºæ–°åŠŸèƒ½æ·»åŠ æµ‹è¯•ç”¨ä¾‹
- ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡
- ä¿æŒæµ‹è¯•è¦†ç›–ç‡

## æ–‡æ¡£æ›´æ–°

- æ›´æ–°ç›¸å…³çš„ API æ–‡æ¡£
- æ·»åŠ ä½¿ç”¨ç¤ºä¾‹
- æ›´æ–° README å’Œå˜æ›´æ—¥å¿—

## å®¡æ ¸æµç¨‹

æ‰€æœ‰ Pull Request éƒ½éœ€è¦ç»è¿‡ä»£ç å®¡æ ¸ï¼š

1. è‡ªåŠ¨åŒ–æ£€æŸ¥é€šè¿‡
2. è‡³å°‘ä¸€ä¸ªç»´æŠ¤è€…å®¡æ ¸
3. è§£å†³æ‰€æœ‰åé¦ˆé—®é¢˜
4. åˆå¹¶åˆ°ä¸»åˆ†æ”¯

## ç¤¾åŒºå‡†åˆ™

- ä¿æŒå‹å¥½å’Œå°Šé‡
- å»ºè®¾æ€§åé¦ˆ
- å¸®åŠ©æ–°è´¡çŒ®è€…
- éµå¾ªè¡Œä¸ºå‡†åˆ™

## è·å¾—å¸®åŠ©

å¦‚æœæ‚¨åœ¨è´¡çŒ®è¿‡ç¨‹ä¸­éœ€è¦å¸®åŠ©ï¼š

- æŸ¥çœ‹æ–‡æ¡£å’Œ FAQ
- åœ¨ Issue ä¸­æé—®
- å‚ä¸ç¤¾åŒºè®¨è®º
- è”ç³»ç»´æŠ¤è€…

å†æ¬¡æ„Ÿè°¢æ‚¨çš„è´¡çŒ®ï¼ğŸ‰
"""

    return content

def generate_markdown_export(repo_url: str, pages: List[WikiPage]) -> str:
    """
    Generate Markdown export of wiki pages.

    Args:
        repo_url: The repository URL
        pages: List of wiki pages

    Returns:
        Markdown content as string
    """
    # Start with metadata
    markdown = f"# Wiki Documentation for {repo_url}\n\n"
    markdown += f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

    # Add table of contents
    markdown += "## Table of Contents\n\n"
    for page in pages:
        markdown += f"- [{page.title}](#{page.id})\n"
    markdown += "\n"

    # Add each page
    for page in pages:
        markdown += f"<a id='{page.id}'></a>\n\n"
        markdown += f"## {page.title}\n\n"



        # Add related pages
        if page.relatedPages and len(page.relatedPages) > 0:
            markdown += "### Related Pages\n\n"
            related_titles = []
            for related_id in page.relatedPages:
                # Find the title of the related page
                related_page = next((p for p in pages if p.id == related_id), None)
                if related_page:
                    related_titles.append(f"[{related_page.title}](#{related_id})")

            if related_titles:
                markdown += "Related topics: " + ", ".join(related_titles) + "\n\n"

        # Add page content
        markdown += f"{page.content}\n\n"
        markdown += "---\n\n"

    return markdown

def generate_json_export(repo_url: str, pages: List[WikiPage]) -> str:
    """
    Generate JSON export of wiki pages.

    Args:
        repo_url: The repository URL
        pages: List of wiki pages

    Returns:
        JSON content as string
    """
    # Create a dictionary with metadata and pages
    export_data = {
        "metadata": {
            "repository": repo_url,
            "generated_at": datetime.now().isoformat(),
            "page_count": len(pages)
        },
        "pages": [page.model_dump() for page in pages]
    }

    # Convert to JSON string with pretty formatting
    return json.dumps(export_data, indent=2)

# Import the simplified chat implementation
from api.simple_chat import chat_completions_stream
from api.websocket_wiki import handle_websocket_chat

# Add the chat_completions_stream endpoint to the main app
app.add_api_route("/chat/completions/stream", chat_completions_stream, methods=["POST"])

# Add the WebSocket endpoint
app.add_websocket_route("/ws/chat", handle_websocket_chat)

# --- Wiki Cache Helper Functions ---

WIKI_CACHE_DIR = os.path.join(get_adalflow_default_root_path(), "wikicache")
os.makedirs(WIKI_CACHE_DIR, exist_ok=True)

def get_wiki_cache_path(owner: str, repo: str, repo_type: str, language: str) -> str:
    """Generates the file path for a given wiki cache."""
    filename = f"deepwiki_cache_{repo_type}_{owner}_{repo}_{language}.json"
    return os.path.join(WIKI_CACHE_DIR, filename)

async def read_wiki_cache(owner: str, repo: str, repo_type: str, language: str) -> Optional[WikiCacheData]:
    """Reads wiki cache data from the file system."""
    cache_path = get_wiki_cache_path(owner, repo, repo_type, language)
    if os.path.exists(cache_path):
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return WikiCacheData(**data)
        except Exception as e:
            logger.error(f"Error reading wiki cache from {cache_path}: {e}")
            return None
    return None

async def save_wiki_cache(data: WikiCacheRequest) -> bool:
    """Saves wiki cache data to the file system."""
    cache_path = get_wiki_cache_path(data.repo.owner, data.repo.repo, data.repo.type, data.language)
    logger.info(f"Attempting to save wiki cache. Path: {cache_path}")
    try:
        payload = WikiCacheData(
            wiki_structure=data.wiki_structure,
            generated_pages=data.generated_pages,
            repo=data.repo,
            provider=data.provider,
            model=data.model
        )
        # Log size of data to be cached for debugging (avoid logging full content if large)
        try:
            payload_json = payload.model_dump_json()
            payload_size = len(payload_json.encode('utf-8'))
            logger.info(f"Payload prepared for caching. Size: {payload_size} bytes.")
        except Exception as ser_e:
            logger.warning(f"Could not serialize payload for size logging: {ser_e}")


        logger.info(f"Writing cache file to: {cache_path}")
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(payload.model_dump(), f, indent=2)
        logger.info(f"Wiki cache successfully saved to {cache_path}")
        return True
    except IOError as e:
        logger.error(f"IOError saving wiki cache to {cache_path}: {e.strerror} (errno: {e.errno})", exc_info=True)
        return False
    except Exception as e:
        logger.error(f"Unexpected error saving wiki cache to {cache_path}: {e}", exc_info=True)
        return False

# --- Wiki Cache API Endpoints ---

@app.get("/api/wiki_cache", response_model=Optional[WikiCacheData])
async def get_cached_wiki(
    owner: str = Query(..., description="Repository owner"),
    repo: str = Query(..., description="Repository name"),
    repo_type: str = Query(..., description="Repository type (e.g., github, gitlab)"),
    language: str = Query(..., description="Language of the wiki content")
):
    """
    Retrieves cached wiki data (structure and generated pages) for a repository.
    """
    # Language validation
    supported_langs = configs["lang_config"]["supported_languages"]
    if not supported_langs.__contains__(language):
        language = configs["lang_config"]["default"]

    logger.info(f"Attempting to retrieve wiki cache for {owner}/{repo} ({repo_type}), lang: {language}")
    cached_data = await read_wiki_cache(owner, repo, repo_type, language)
    if cached_data:
        return cached_data
    else:
        # Return 200 with null body if not found, as frontend expects this behavior
        # Or, raise HTTPException(status_code=404, detail="Wiki cache not found") if preferred
        logger.info(f"Wiki cache not found for {owner}/{repo} ({repo_type}), lang: {language}")
        return None

@app.post("/api/wiki_cache")
async def store_wiki_cache(request_data: WikiCacheRequest):
    """
    Stores generated wiki data (structure and pages) to the server-side cache.
    """
    # Language validation
    supported_langs = configs["lang_config"]["supported_languages"]

    if not supported_langs.__contains__(request_data.language):
        request_data.language = configs["lang_config"]["default"]

    logger.info(f"Attempting to save wiki cache for {request_data.repo.owner}/{request_data.repo.repo} ({request_data.repo.type}), lang: {request_data.language}")
    success = await save_wiki_cache(request_data)
    if success:
        return {"message": "Wiki cache saved successfully"}
    else:
        raise HTTPException(status_code=500, detail="Failed to save wiki cache")

@app.delete("/api/wiki_cache")
async def delete_wiki_cache(
    owner: str = Query(..., description="Repository owner"),
    repo: str = Query(..., description="Repository name"),
    repo_type: str = Query(..., description="Repository type (e.g., github, gitlab)"),
    language: str = Query(..., description="Language of the wiki content"),
    authorization_code: Optional[str] = Query(None, description="Authorization code")
):
    """
    Deletes a specific wiki cache from the file system.
    """
    # Language validation
    supported_langs = configs["lang_config"]["supported_languages"]
    if not supported_langs.__contains__(language):
        raise HTTPException(status_code=400, detail="Language is not supported")

    if WIKI_AUTH_MODE:
        logger.info("check the authorization code")
        if not authorization_code or WIKI_AUTH_CODE != authorization_code:
            raise HTTPException(status_code=401, detail="Authorization code is invalid")

    logger.info(f"Attempting to delete wiki cache for {owner}/{repo} ({repo_type}), lang: {language}")
    cache_path = get_wiki_cache_path(owner, repo, repo_type, language)

    if os.path.exists(cache_path):
        try:
            os.remove(cache_path)
            logger.info(f"Successfully deleted wiki cache: {cache_path}")
            return {"message": f"Wiki cache for {owner}/{repo} ({language}) deleted successfully"}
        except Exception as e:
            logger.error(f"Error deleting wiki cache {cache_path}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to delete wiki cache: {str(e)}")
    else:
        logger.warning(f"Wiki cache not found, cannot delete: {cache_path}")
        raise HTTPException(status_code=404, detail="Wiki cache not found")

@app.get("/health")
async def health_check():
    """Health check endpoint for Docker and monitoring"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "deepwiki-api"
    }

@app.get("/")
async def root():
    """Root endpoint to check if the API is running and list available endpoints dynamically."""
    # Collect routes dynamically from the FastAPI app
    endpoints = {}
    for route in app.routes:
        if hasattr(route, "methods") and hasattr(route, "path"):
            # Skip docs and static routes
            if route.path in ["/openapi.json", "/docs", "/redoc", "/favicon.ico"]:
                continue
            # Group endpoints by first path segment
            path_parts = route.path.strip("/").split("/")
            group = path_parts[0].capitalize() if path_parts[0] else "Root"
            method_list = list(route.methods - {"HEAD", "OPTIONS"})
            for method in method_list:
                endpoints.setdefault(group, []).append(f"{method} {route.path}")

    # Optionally, sort endpoints for readability
    for group in endpoints:
        endpoints[group].sort()

    return {
        "message": "Welcome to Streaming API",
        "version": "1.0.0",
        "endpoints": endpoints
    }

# --- Processed Projects Endpoint --- (New Endpoint)
@app.get("/api/processed_projects", response_model=List[ProcessedProjectEntry])
async def get_processed_projects():
    """
    Lists all processed projects found in the wiki cache directory.
    Projects are identified by files named like: deepwiki_cache_{repo_type}_{owner}_{repo}_{language}.json
    """
    project_entries: List[ProcessedProjectEntry] = []
    # WIKI_CACHE_DIR is already defined globally in the file

    try:
        if not os.path.exists(WIKI_CACHE_DIR):
            logger.info(f"Cache directory {WIKI_CACHE_DIR} not found. Returning empty list.")
            return []

        logger.info(f"Scanning for project cache files in: {WIKI_CACHE_DIR}")
        filenames = await asyncio.to_thread(os.listdir, WIKI_CACHE_DIR) # Use asyncio.to_thread for os.listdir

        for filename in filenames:
            if filename.startswith("deepwiki_cache_") and filename.endswith(".json"):
                file_path = os.path.join(WIKI_CACHE_DIR, filename)
                try:
                    stats = await asyncio.to_thread(os.stat, file_path) # Use asyncio.to_thread for os.stat
                    parts = filename.replace("deepwiki_cache_", "").replace(".json", "").split('_')

                    # Expecting repo_type_owner_repo_language
                    # Example: deepwiki_cache_github_AsyncFuncAI_deepwiki-open_en.json
                    # parts = [github, AsyncFuncAI, deepwiki-open, en]
                    if len(parts) >= 4:
                        repo_type = parts[0]
                        owner = parts[1]
                        language = parts[-1] # language is the last part
                        repo = "_".join(parts[2:-1]) # repo can contain underscores

                        project_entries.append(
                            ProcessedProjectEntry(
                                id=filename,
                                owner=owner,
                                repo=repo,
                                name=f"{owner}/{repo}",
                                repo_type=repo_type,
                                submittedAt=int(stats.st_mtime * 1000), # Convert to milliseconds
                                language=language
                            )
                        )
                    else:
                        logger.warning(f"Could not parse project details from filename: {filename}")
                except Exception as e:
                    logger.error(f"Error processing file {file_path}: {e}")
                    continue # Skip this file on error

        # Sort by most recent first
        project_entries.sort(key=lambda p: p.submittedAt, reverse=True)
        logger.info(f"Found {len(project_entries)} processed project entries.")
        return project_entries

    except Exception as e:
        logger.error(f"Error listing processed projects from {WIKI_CACHE_DIR}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to list processed projects from server cache.")
